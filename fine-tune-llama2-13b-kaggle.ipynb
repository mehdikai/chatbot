{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T11:34:50.228724Z",
     "iopub.status.busy": "2024-05-14T11:34:50.228418Z",
     "iopub.status.idle": "2024-05-14T11:34:51.356011Z",
     "shell.execute_reply": "2024-05-14T11:34:51.354727Z",
     "shell.execute_reply.started": "2024-05-14T11:34:50.228698Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May 14 11:34:51 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   38C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla T4                       Off | 00000000:00:05.0 Off |                    0 |\n",
      "| N/A   40C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T11:34:56.411463Z",
     "iopub.status.busy": "2024-05-14T11:34:56.411044Z",
     "iopub.status.idle": "2024-05-14T11:40:30.933357Z",
     "shell.execute_reply": "2024-05-14T11:40:30.931761Z",
     "shell.execute_reply.started": "2024-05-14T11:34:56.411426Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --upgrade torch==2.2.2  # xformers 0.0.25.post1 requires torch==2.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T11:49:51.673035Z",
     "iopub.status.busy": "2024-05-14T11:49:51.672535Z",
     "iopub.status.idle": "2024-05-14T11:49:53.198173Z",
     "shell.execute_reply": "2024-05-14T11:49:53.197178Z",
     "shell.execute_reply.started": "2024-05-14T11:49:51.672996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__) # be sure that the version of torch 2.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T11:49:55.952498Z",
     "iopub.status.busy": "2024-05-14T11:49:55.951630Z",
     "iopub.status.idle": "2024-05-14T11:50:55.009018Z",
     "shell.execute_reply": "2024-05-14T11:50:55.007717Z",
     "shell.execute_reply.started": "2024-05-14T11:49:55.952460Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps \"xformers<0.0.26\" trl peft accelerate bitsandbytes triton # we need old xformers <0.26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T11:54:49.188330Z",
     "iopub.status.busy": "2024-05-14T11:54:49.187897Z",
     "iopub.status.idle": "2024-05-14T11:55:07.833528Z",
     "shell.execute_reply": "2024-05-14T11:55:07.832613Z",
     "shell.execute_reply.started": "2024-05-14T11:54:49.188293Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-14 11:54:58.600005: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-14 11:54:58.600103: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-14 11:54:58.766874: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from unsloth import FastLanguageModel # we use unsloth technique to optimise training \n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T11:55:10.722981Z",
     "iopub.status.busy": "2024-05-14T11:55:10.722285Z",
     "iopub.status.idle": "2024-05-14T11:55:10.728092Z",
     "shell.execute_reply": "2024-05-14T11:55:10.726966Z",
     "shell.execute_reply.started": "2024-05-14T11:55:10.722948Z"
    }
   },
   "outputs": [],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"unsloth/llama-2-13b-bnb-4bit\"\n",
    "\n",
    "# The instruction dataset to use\n",
    "dataset_name = \"bragour/Palestinian_Truth_English\"\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = \"Camel-13b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T11:55:12.796239Z",
     "iopub.status.busy": "2024-05-14T11:55:12.795820Z",
     "iopub.status.idle": "2024-05-14T11:55:15.479776Z",
     "shell.execute_reply": "2024-05-14T11:55:15.478985Z",
     "shell.execute_reply.started": "2024-05-14T11:55:12.796208Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aacfde80dcda4b8db2bd8426dbac8af3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 27.1M/27.1M [00:00<00:00, 63.2MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a09394a11d9411b96c2cb9267b26171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load dataset (you can process it here)\n",
    "dataset = load_dataset(dataset_name, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T11:55:30.857968Z",
     "iopub.status.busy": "2024-05-14T11:55:30.856848Z",
     "iopub.status.idle": "2024-05-14T11:55:30.873587Z",
     "shell.execute_reply": "2024-05-14T11:55:30.872646Z",
     "shell.execute_reply.started": "2024-05-14T11:55:30.857926Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '<s>[INST]co published with the institute for palestine studies 2022 muhammad ali khalidi published by or books new york and london visit our website at orbooks com all rights information rights orbooks com all rights reserved no part of[/INST]this book may be reproduced or transmitted in any form or by any means electronic or mechanical including photocopy recording or any information storage retrieval system without permission in writing from the publisher except brief passages for review purposes first printing 2022 cataloging in publication data is available from the library of congress a catalog record for this book is available from the british library typeset by lapiz digital services printed by bookmobile usa and cpi uk paperback isbn 978 1 68219 347 1 ebook isbn 978 1 68219 348 8 contents translator s note and acknowledgments introduction</s>'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0] # to see the form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T11:55:37.124458Z",
     "iopub.status.busy": "2024-05-14T11:55:37.123721Z",
     "iopub.status.idle": "2024-05-14T11:55:37.129133Z",
     "shell.execute_reply": "2024-05-14T11:55:37.127967Z",
     "shell.execute_reply.started": "2024-05-14T11:55:37.124425Z"
    }
   },
   "outputs": [],
   "source": [
    "dtype = None # -----------None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "\n",
    "load_in_4bit = True #---- Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "max_seq_length = 2048 #-- Choose any! unsloth auto support RoPE Scaling internally!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T11:56:32.807513Z",
     "iopub.status.busy": "2024-05-14T11:56:32.806824Z",
     "iopub.status.idle": "2024-05-14T11:57:17.011122Z",
     "shell.execute_reply": "2024-05-14T11:57:17.009869Z",
     "shell.execute_reply.started": "2024-05-14T11:56:32.807481Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c5d30e05002471aa798486c321ea717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.09k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.5\n",
      "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.2+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. Xformers = 0.0.25.post1. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "344ff2ffec2147ef89e00b812a6c0dc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/7.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc3f76a991b4360a644c8c3fe3c3d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/183 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973df98f6da34d1d836fdf1179f59c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/894 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d62972249ced493eb185419de7db6fbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ecfc853f8694785ace21fcf7521c4b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f045011f72674b4297477d5a7aff268f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    #token=\"\" # hugging face READ token or use unsloth models\n",
    ")\n",
    "\n",
    "#model = AutoModelForCausalLM.from_pretrained(\n",
    "    #base_model,\n",
    "    #token=\"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\",\n",
    "    #quantization_config=quant_config,\n",
    "    #device_map={\"\": 0}\n",
    "#)\n",
    "#model.config.use_cache = False\n",
    "#model.config.pretraining_tp = 1###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T11:57:28.317678Z",
     "iopub.status.busy": "2024-05-14T11:57:28.316870Z",
     "iopub.status.idle": "2024-05-14T11:57:29.530109Z",
     "shell.execute_reply": "2024-05-14T11:57:29.529370Z",
     "shell.execute_reply.started": "2024-05-14T11:57:28.317643Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.5 patched 40 layers with 40 QKV layers, 40 O layers and 40 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    #  \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T11:57:48.977634Z",
     "iopub.status.busy": "2024-05-14T11:57:48.976564Z",
     "iopub.status.idle": "2024-05-14T11:58:00.217866Z",
     "shell.execute_reply": "2024-05-14T11:58:00.216636Z",
     "shell.execute_reply.started": "2024-05-14T11:57:48.977588Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:246: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11ad05a95aa040b8b815d4092c2f6f1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/29939 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, #------------------------------- Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,#------------ try to keep it small\n",
    "        gradient_accumulation_steps = 4, #----------- Gradient\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 300, #-----------------------------------------------------------------changiw ady 3la 7sab lerror-- steps\n",
    "        learning_rate = 2e-4, #---------------------- LR\n",
    "        fp16 = not torch.cuda.is_bf16_supported(), #- False is also possible\n",
    "        bf16 = torch.cuda.is_bf16_supported(),     #- False is also possible\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\", #--------------------- adamw_8bit worked for me ;)\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\", \n",
    "        seed = 3407,\n",
    "        report_to=\"tensorboard\",\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T11:58:04.814037Z",
     "iopub.status.busy": "2024-05-14T11:58:04.813547Z",
     "iopub.status.idle": "2024-05-14T11:58:04.820764Z",
     "shell.execute_reply": "2024-05-14T11:58:04.819874Z",
     "shell.execute_reply.started": "2024-05-14T11:58:04.814000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = Tesla T4. Max memory = 14.748 GB.\n",
      "7.037 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T11:58:07.260201Z",
     "iopub.status.busy": "2024-05-14T11:58:07.259260Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 29,939 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 60\n",
      " \"-____-\"     Number of trainable parameters = 62,586,880\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/60 02:07 < 09:05, 0.09 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.513100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.869400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.243000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.785100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.850500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.131600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.633700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.979100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.690800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.541400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.741600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train() # the scary parts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-13T17:38:47.746238Z",
     "iopub.status.idle": "2024-05-13T17:38:47.746570Z",
     "shell.execute_reply": "2024-05-13T17:38:47.746425Z",
     "shell.execute_reply.started": "2024-05-13T17:38:47.746411Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "#model.save_pretrained(\"lora_model\") # Local saving\n",
    "model.push_to_hub(\"Zak587/lora_model\", token=\"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\") # Online saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-13T17:38:47.748057Z",
     "iopub.status.idle": "2024-05-13T17:38:47.748357Z",
     "shell.execute_reply": "2024-05-13T17:38:47.748222Z",
     "shell.execute_reply.started": "2024-05-13T17:38:47.748209Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-13T17:38:47.749268Z",
     "iopub.status.idle": "2024-05-13T17:38:47.749608Z",
     "shell.execute_reply": "2024-05-13T17:38:47.749463Z",
     "shell.execute_reply.started": "2024-05-13T17:38:47.749450Z"
    }
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir results/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-13T17:38:47.751455Z",
     "iopub.status.idle": "2024-05-13T17:38:47.751783Z",
     "shell.execute_reply": "2024-05-13T17:38:47.751639Z",
     "shell.execute_reply.started": "2024-05-13T17:38:47.751625Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Run text generation pipeline with our next model\n",
    "prompt = \"what is capital of israel ?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=24)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-13T17:38:47.752951Z",
     "iopub.status.idle": "2024-05-13T17:38:47.753272Z",
     "shell.execute_reply": "2024-05-13T17:38:47.753127Z",
     "shell.execute_reply.started": "2024-05-13T17:38:47.753114Z"
    }
   },
   "outputs": [],
   "source": [
    "# Empty VRAM\n",
    "del model\n",
    "del pipe\n",
    "del trainer\n",
    "import gc\n",
    "gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-13T17:38:47.755361Z",
     "iopub.status.idle": "2024-05-13T17:38:47.755723Z",
     "shell.execute_reply": "2024-05-13T17:38:47.755580Z",
     "shell.execute_reply.started": "2024-05-13T17:38:47.755565Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-13T17:38:47.757085Z",
     "iopub.status.idle": "2024-05-13T17:38:47.757413Z",
     "shell.execute_reply": "2024-05-13T17:38:47.757248Z",
     "shell.execute_reply.started": "2024-05-13T17:38:47.757235Z"
    }
   },
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-13T17:38:47.759094Z",
     "iopub.status.idle": "2024-05-13T17:38:47.759464Z",
     "shell.execute_reply": "2024-05-13T17:38:47.759280Z",
     "shell.execute_reply.started": "2024-05-13T17:38:47.759266Z"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-13T17:38:47.760238Z",
     "iopub.status.idle": "2024-05-13T17:38:47.760586Z",
     "shell.execute_reply": "2024-05-13T17:38:47.760433Z",
     "shell.execute_reply.started": "2024-05-13T17:38:47.760417Z"
    }
   },
   "outputs": [],
   "source": [
    "model.push_to_hub(\"bragour/Camel-13b-chat\", check_pr=True)\n",
    "\n",
    "tokenizer.push_to_hub(\"bragour/Camel-13b-chat\",check_pr=True)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
